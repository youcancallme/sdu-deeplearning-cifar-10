{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youcancallme/sdu-deeplearning-cifar-10/blob/main/CIFAR-10-v7-v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDkwnu6X4el-"
      },
      "source": [
        "# 初始化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDKs8U2z4el_"
      },
      "source": [
        "## 本地实现"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n"
      ],
      "metadata": {
        "id": "Ahm_QxWtFmLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuPONzSXfdg1"
      },
      "source": [
        "首先进行的是文件读取"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j01-fgRTfjeR"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkP6KD4j4emC"
      },
      "source": [
        "## torchvision实现"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KumQxCz64emC"
      },
      "source": [
        "上面是我自己对数据的处理，其实torchvision提供了对cifar-10数据集的处理，并且提供了及其方便的预处理代码"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboardX\n",
        "from tensorboardX import SummaryWriter"
      ],
      "metadata": {
        "id": "P9w9cotKCJIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmu5X4aM4emD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6MeWu1R4emD"
      },
      "source": [
        "### 数据增强"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIj2nYmM4emD"
      },
      "source": [
        "图像增广，训练集进行增广，测试时标准化执行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sw16QZnV4emD"
      },
      "outputs": [],
      "source": [
        "#已经解压的数据集路径\n",
        "#本地\n",
        "# unzip_folder_path='../cifar-10-python'\n",
        "#colab\n",
        "unzip_folder_path= '/content/drive/MyDrive/cifar-10-python'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c7rN2oL4emD"
      },
      "outputs": [],
      "source": [
        "#训练集\n",
        "transform_train = torchvision.transforms.Compose([\n",
        "    # 在高度和宽度上将图像放大到40像素的正方形\n",
        "    torchvision.transforms.Resize(40),\n",
        "    # 生成一个面积为原始图像面积0.64～1倍的小正方形，\n",
        "    # 然后将其缩放为高度和宽度均为32像素的正方形\n",
        "    torchvision.transforms.RandomResizedCrop(32, scale=(0.64, 1.0),ratio=(1.0, 1.0)),\n",
        "    #以 50% 的概率对输入图像进行水平翻转,用于数据增强。\n",
        "    torchvision.transforms.RandomHorizontalFlip(),\n",
        "    # NumPy 数组转换为 PyTorch Tensor 。\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    # 标准化图像的每个通道 三通道分别归一化，到均值为 0、标准差为 1 的分布。\n",
        "    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
        "                                     [0.2023, 0.1994, 0.2010])])\n",
        "\n",
        "#测试集\n",
        "transform_test = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
        "                                     [0.2023, 0.1994, 0.2010])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xymu_dC04emD"
      },
      "outputs": [],
      "source": [
        "#如果不使用数据增广的话，只将数据变为tensor和标准化\n",
        "transform= torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize([0.4914, 0.4822, 0.4465],\n",
        "                                     [0.2023, 0.1994, 0.2010])])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJxK43n64emD"
      },
      "outputs": [],
      "source": [
        "#训练集 train=True加载训练集\n",
        "trainset = torchvision.datasets.CIFAR10(root=unzip_folder_path, train=True,\n",
        "                                        download=False, transform=transform_train)\n",
        "\n",
        "#测试集\n",
        "testset = torchvision.datasets.CIFAR10(root=unzip_folder_path, train=False,\n",
        "                                       download=False, transform=transform_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DIW8SNS_C7H"
      },
      "outputs": [],
      "source": [
        "image, label = trainset[0]\n",
        "image_size = image.size()\n",
        "image_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6oZQvPX4emD"
      },
      "source": [
        "### 绘图准备"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the summary writer for TensorBoard\n",
        "writer = SummaryWriter('/content/drive/MyDrive/runs')"
      ],
      "metadata": {
        "id": "-ZWj3sO1Cd_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7aMuYZH4emE"
      },
      "source": [
        "## 1折交叉验证"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCXrvNlI4emE"
      },
      "source": [
        "使用KFold类从sklearn.model_selection库中创建了一个1折交叉验证对象。n_splits参数设置为10，表示将数据集划分为10个折（10份），shuffle=True表示在划分前对数据进行洗牌。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Et9cwLFF4emE"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits=10, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcZ0r2nq4emE"
      },
      "outputs": [],
      "source": [
        "# argmax 函数别名定义了一个匿名函数，它接受参数 x，并调用 x.argmax(*args, **kwargs) 方法。\n",
        "# astype 函数别名定义了一个匿名函数，它接受参数 x，并调用 x.type(*args, **kwargs) 方法。\n",
        "# reduce_sum 函数别名定义了一个匿名函数，它接受参数 x，并调用 x.sum(*args, **kwargs) 方法。\n",
        "\n",
        "argmax = lambda x, *args, **kwargs: x.argmax(*args, **kwargs)\n",
        "astype = lambda x, *args, **kwargs: x.type(*args, **kwargs)\n",
        "reduce_sum = lambda x, *args, **kwargs: x.sum(*args, **kwargs)\n",
        "def accuracy(y_hat, y):\n",
        "    #首先处理y_hat形状，如果 y_hat 的维度大于1且最后一个维度大于1,则说明 y_hat 是一个包含多个类别概率的tensor。\n",
        "    #这种情况下,需要使用 argmax 函数找出每个样本预测概率最高的类别索引,赋值给 y_hat。\n",
        "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
        "        y_hat = argmax(y_hat, axis=1)\n",
        "  #使用astype函数将y_hat的数据类型转换为和y相同的类型,方便比较\n",
        "  #比较是否相等，如果相等就得到布尔类型的tensor\n",
        "    cmp = astype(y_hat, y.dtype) == y\n",
        "  #使用 reduce_sum 函数统计 cmp 中为True的元素个数,得到预测正确的样本数\n",
        "    return float(reduce_sum(astype(cmp, y.dtype)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5Pzk1fA4emE"
      },
      "outputs": [],
      "source": [
        "def train_batch(net, X, y, loss, trainer, devices):\n",
        "    # 转移到一个device上\n",
        "    if isinstance(X, list):\n",
        "        # Required for BERT fine-tuning (to be covered later)\n",
        "        X = [x.to(devices[0]) for x in X]\n",
        "    else:\n",
        "        X = X.to(devices[0])\n",
        "    y = y.to(devices[0])\n",
        "    #训练\n",
        "    net.train()\n",
        "    #梯度为0\n",
        "    trainer.zero_grad()\n",
        "    #前向传播\n",
        "    pred = net(X)\n",
        "    #计算损失\n",
        "    l = loss(pred, y)\n",
        "    #反向传播更新参数\n",
        "    l.sum().backward()\n",
        "    trainer.step()\n",
        "\n",
        "    #本批次的损失和准确度\n",
        "    train_loss_sum = l.sum()\n",
        "    train_acc_sum = accuracy(pred, y)\n",
        "    return train_loss_sum, train_acc_sum\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "train_accs = []\n",
        "val_accs = []"
      ],
      "metadata": {
        "id": "t_KGrqUGNqq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#用于训练：\n",
        "def train(train_iter,valid_iter,net, criterion, lr_period, lr_decay, lr, wd, devices, num_epochs=1, batch_size=128):\n",
        "  # 为每一折创建一个新的网络实例\n",
        "    model = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
        "\n",
        "      # sgd优化器\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
        "    #学习率调度器，\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_period, lr_decay)\n",
        "    #计算k折的平均结果\n",
        "    cur_train_losses = 0.0\n",
        "    cur_train_accs = 0.0\n",
        "    cur_val_accs = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "\n",
        "        #running_loss 用于记录整个训练过程中的累积损失值。\n",
        "        running_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        model.train()\n",
        "\n",
        "        #循环训练集的特征和标签\n",
        "        for i, (inputs, labels) in enumerate(train_iter):\n",
        "          #计算每个批次的损失和返回预测正确的样本数，\n",
        "          #net 特征 标签 损失函数 sgd优化器 设备\n",
        "            loss, acc = train_batch(model, inputs, labels, criterion, optimizer, devices)\n",
        "            #用正确的样本数/当前batch的总样本数即可\n",
        "            Acc=float(acc) / labels.shape[0]\n",
        "            train_correct += acc\n",
        "            train_total += labels.shape[0]\n",
        "          #将当前批次的损失值 loss 加到 running_loss 变量中。\n",
        "            running_loss += loss.item()\n",
        "          #检查当前批次的索引是否为训练数据批次总数的 1/5 倍,或者是否为最后一个批次。\n",
        "            if (i + 1) % (len(train_iter) // 5) == 0 or i == len(train_iter) - 1:\n",
        "              #当前batch的索引和总batch数量，当前的平均损失，当前的准确率\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}]，Batch [{i+1}/{len(train_iter)}], Loss: {running_loss / (i + 1):.4f}, Acc: {Acc:.2f}')\n",
        "                # Dynamic plotting\n",
        "                writer.add_scalar('Loss/train', loss.item(), epoch * len(train_iter) + i)\n",
        "                writer.add_scalar('Accuracy/train', Acc, epoch * len(train_iter) + i)\n",
        "        train_losses.append(running_loss /len(train_iter))\n",
        "        cur_train_losses=(cur_train_losse+running_loss /len(train_iter))/epoch\n",
        "        train_accs.append(train_correct/train_total)\n",
        "        cur_val_accs=(cur_val_accs+train_correct/train_total)/epoch\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "    # with torch.no_grad():\n",
        "    if valid_iter is not None:\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in valid_iter:\n",
        "            inputs = inputs.to(devices[0])\n",
        "            labels = labels.to(devices[0])\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f'Validation Accuracy: {accuracy:.2f}%')\n",
        "        writer.add_scalar('Accuracy/val', accuracy, epoch)\n",
        "        val_accs.append(accuracy)\n",
        "        cur_val_accs = accuracy\n",
        "\n",
        "# 最后得到是当前这一折损失 训练和验证的正确率\n",
        "    return cur_train_losses,cur_train_accs,cur_val_accs\n"
      ],
      "metadata": {
        "id": "CG1-AGv1H1gS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#可以用于验证集测试\n",
        "def k_fold(criterion, lr_period, lr_decay, lr, wd, trainset,devices, num_epochs=1, batch_size=128):\n",
        "    # 使用 KFold 进行 10 折交叉验证，shuffle=True 表示打乱数据集\n",
        "    kfold = KFold(n_splits=10, shuffle=True)\n",
        "\n",
        "    for fold, (train_indices, val_indices) in enumerate(kfold.split(trainset)):\n",
        "        cur_train_losses = 0.0\n",
        "        cur_train_accs = 0.0\n",
        "        cur_val_accs = 0.0\n",
        "        print(f'Fold {fold+1}')\n",
        "        trainset_fold = torch.utils.data.Subset(trainset, train_indices)  # 获取当前折的训练集\n",
        "        valset_fold = torch.utils.data.Subset(trainset, val_indices)  # 获取当前折的验证集\n",
        "        net=get_net()\n",
        "\n",
        "        #当前折训练数据集\n",
        "        trainloader_fold = torch.utils.data.DataLoader(trainset_fold, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "        #当前折验证数据集\n",
        "        valloader_fold = torch.utils.data.DataLoader(valset_fold, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "        cur_train_losses,cur_train_accs,cur_val_accs=train(trainloader_fold, valloader_fold,net, criterion, lr_period, lr_decay, lr, wd, devices)\n",
        "\n",
        "        print(f'Fold {fold+1} AvgLoss: {cur_train_losses}, TraninAvgAcc: {cur_train_accs} ,ValAvgAcc: {cur_val_accs}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LQdtxymQHmOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2swg6sg4emE"
      },
      "source": [
        "## ResNet模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc_FCUk-4emF"
      },
      "source": [
        "残差块"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuZxJ8Zq4emF"
      },
      "outputs": [],
      "source": [
        "from torch.nn import functional as F\n",
        "#画图表示\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, input_channels, num_channels,\n",
        "                 use_1x1conv=False, strides=1):\n",
        "        #初始化\n",
        "        super().__init__()\n",
        "        #两个卷积层，内核大小都为3，\n",
        "        self.conv1 = nn.Conv2d(input_channels, num_channels,\n",
        "                               kernel_size=3, padding=1, stride=strides)\n",
        "        #输入通道为上一层的输出通道\n",
        "        self.conv2 = nn.Conv2d(num_channels, num_channels,\n",
        "                               kernel_size=3, padding=1)\n",
        "        if use_1x1conv:#如果不使用的话，在应用Relu前，将输入添加到输出；如果使用，通过添加1×1卷积来调整通道和分辨率\n",
        "            self.conv3 = nn.Conv2d(input_channels, num_channels,\n",
        "                                   kernel_size=1, stride=strides)\n",
        "        else:\n",
        "            self.conv3 = None\n",
        "        #两个批量归一化层\n",
        "        self.bn1 = nn.BatchNorm2d(num_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(num_channels)\n",
        "\n",
        "    def forward(self, X):\n",
        "        Y = F.relu(self.bn1(self.conv1(X)))\n",
        "        Y = self.bn2(self.conv2(Y))\n",
        "        if self.conv3:\n",
        "            X = self.conv3(X)\n",
        "        Y += X\n",
        "        return F.relu(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5i_NbFU4emF"
      },
      "outputs": [],
      "source": [
        "def resnet18(num_classes, in_channels=1):\n",
        "\n",
        "    def resnet_block(in_channels, out_channels, num_residuals,\n",
        "                     first_block=False):\n",
        "        blk = []\n",
        "        for i in range(num_residuals):\n",
        "            if i == 0 and not first_block:\n",
        "                blk.append(\n",
        "                    Residual(in_channels, out_channels, use_1x1conv=True,\n",
        "                                 strides=2))\n",
        "            else:\n",
        "                blk.append(Residual(out_channels, out_channels))\n",
        "        return nn.Sequential(*blk)\n",
        "\n",
        "    # This model uses a smaller convolution kernel, stride, and padding and\n",
        "    #不可以删掉最大池化层，可能会使梯度消失，使网络的感受野减小\n",
        "    net = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),\n",
        "        nn.BatchNorm2d(64), nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "    net.add_module(\"resnet_block1\", resnet_block(64, 64, 2, first_block=True))\n",
        "    net.add_module(\"resnet_block2\", resnet_block(64, 128, 2))\n",
        "    net.add_module(\"resnet_block3\", resnet_block(128, 256, 2))\n",
        "    net.add_module(\"resnet_block4\", resnet_block(256, 512, 2))\n",
        "    net.add_module(\"global_avg_pool\", nn.AdaptiveAvgPool2d((1, 1)))\n",
        "    net.add_module(\"fc\",nn.Sequential(nn.Flatten(), nn.Linear(512, num_classes)))\n",
        "    return net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giWmXSrU4emF"
      },
      "outputs": [],
      "source": [
        "def get_net():\n",
        "    num_classes = 10\n",
        "    net = resnet18(num_classes, 3)\n",
        "    return net\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(reduction=\"none\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-anj7-7q_C7J"
      },
      "outputs": [],
      "source": [
        "X = torch.rand(size=(4, 3, 32, 32))\n",
        "net=get_net()\n",
        "for layer in net:\n",
        "    X = layer(X)\n",
        "    print(layer.__class__.__name__,'output shape:\\t', X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib"
      ],
      "metadata": {
        "id": "WyIvt_FKD0IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "swb4MIRTD0sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoJKIujw6YOS"
      },
      "outputs": [],
      "source": [
        "def try_all_gpus():\n",
        "    \"\"\"Return all available GPUs, or [cpu(),] if no GPU exists.\"\"\"\n",
        "    devices = [\n",
        "        torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]\n",
        "    return devices if devices else [torch.device('cpu')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR7u-fHK4emF"
      },
      "outputs": [],
      "source": [
        "# 训练：\n",
        "device,num_epochs, lr, wd =try_all_gpus(),20, 2e-4, 5e-4\n",
        "lr_period, lr_decay = 4, 0.9\n",
        "# train(net, train_iter, valid_iter, num_epochs, lr, wd, devices, lr_period,lr_decay)\n",
        "k_fold(criterion, lr_period, lr_decay, lr, wd, trainset,device)\n",
        "\n",
        "# Close the summary writer\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TensorBoard notebook extension.\n",
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir /content/drive/MyDrive/runs"
      ],
      "metadata": {
        "id": "dAlu9Z8rJ2yP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer_eva = SummaryWriter()"
      ],
      "metadata": {
        "id": "LnGrmaa8GOH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader, devices):\n",
        "    \"\"\"\n",
        "    在完整的验证集上评估模型性能\n",
        "\n",
        "    参数:\n",
        "    model (torch.nn.Module): 训练好的模型\n",
        "    testset (torch.utils.data.Dataset): 验证数据集\n",
        "    device (torch.device): 运行设备(CPU或GPU)\n",
        "\n",
        "    返回:\n",
        "    float: 模型在验证集上的性能指标(例如准确率)\n",
        "    \"\"\"\n",
        "    model = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
        "\n",
        "    model.eval()  # 设置模型为评估模式\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # test_loader = DataLoader(testset, batch_size=128, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            if isinstance(data, list):\n",
        "              # Required for BERT fine-tuning (to be covered later)\n",
        "              data = [x.to(devices[0]) for x in data]\n",
        "            else:\n",
        "                data = data.to(devices[0])\n",
        "            target =  target.to(devices[0])\n",
        "            output = model(data)\n",
        "\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy: {accuracy:.2f}%')\n",
        "    writer_eva.add_scalar('Accuracy/val', accuracy)\n",
        "    # return accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "RE3M5v5ELoUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# net=get_net()\n",
        "test_loader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=True, num_workers=2)\n",
        "evaluate(net, test_loader, device)\n",
        "# writer_eva.close()"
      ],
      "metadata": {
        "id": "Tc4ELE8HM2wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_train_val(train_losses, train_accs, val_losses, val_accs):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "    # 绘制训练和验证的损失\n",
        "    ax1.plot(train_losses, label='Train Loss')\n",
        "    ax1.plot(val_losses, label='Val Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "    ax1.legend()\n",
        "\n",
        "    # 绘制训练和验证的准确率\n",
        "    ax2.plot(train_accs, label='Train Acc')\n",
        "    ax2.plot(val_accs, label='Val Acc')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.set_title('Training and Validation Accuracy')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DZhGksdOPZHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 绘制训练和验证的准确率和损失\n",
        "plot_train_val(train_losses, train_accs, val_losses, val_accs)\n"
      ],
      "metadata": {
        "id": "lUy57UoGPWle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard"
      ],
      "metadata": {
        "id": "sGtIzuPzLhJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir /content/drive/MyDrive/runs"
      ],
      "metadata": {
        "id": "6Gvn-6bjLpNt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "d2l",
      "language": "python",
      "name": "d2l"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}